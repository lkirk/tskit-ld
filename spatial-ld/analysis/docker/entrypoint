#!/usr/bin/env python
import os

# limit number of threads that polars uses
os.environ["POLARS_MAX_THREADS"] = "17"
os.environ["OMP_NUM_THREADS"] = "17"

import argparse
import logging
from pathlib import Path

import numpy as np
import polars as pl
from more_itertools import one

from spatial import (
    compute_divergence_and_geog_distance_for_sim,
    ld_decay,
    ld_decay_two_way,
    load_ts,
    read_parquet_file,
)

logging.basicConfig(format="%(asctime)s %(levelname)s %(message)s", level=logging.INFO)


def parent_exists(value: str) -> Path:
    p = Path(value)
    if not (n := p.parent).exists():
        raise argparse.ArgumentTypeError(f"{n} does not exist")
    if not (n := p.parent).is_dir():
        raise argparse.ArgumentTypeError(f"{n} is not a directory")
    if (n := p).exists():
        raise argparse.ArgumentTypeError(f"{n} exists")
    return p


def file_exists(value: str) -> Path:
    p = Path(value)
    if not (n := p).exists():
        raise argparse.ArgumentTypeError(f"{n} does not exist")
    return p


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser()
    subparsers = parser.add_subparsers(
        title="analyses",
        description="analyses to run",
        dest="subcommand",
        help="analysis step in the pipeline",
        required=True,
    )

    cd = subparsers.add_parser(
        "compute_divergence", help="compute divergence and geographic distance"
    )
    cd.add_argument("in_ts", type=file_exists, help="input tree sequence")
    cd.add_argument("in_df", type=file_exists, help="input data frame")
    cd.add_argument("out_df", type=parent_exists, help="output data frame")

    prep_cd = subparsers.add_parser(
        "prepare_compute_divergence", help="prepare cluster inputs"
    )
    prep_cd.add_argument("in_df", type=file_exists, help="input data frame")
    prep_cd.add_argument(
        "out_dir", type=parent_exists, help="path to save input dataframes"
    )

    prep_decay = subparsers.add_parser(
        "prepare_ld_decay", help="prepare cluster inputs"
    )
    prep_decay.add_argument("in_df", type=file_exists, help="input data frame")
    prep_decay.add_argument(
        "out_dir", type=parent_exists, help="path to save input npz"
    )

    prep_decay_2_pop = subparsers.add_parser(
        "prepare_ld_decay_2_pop", help="prepare cluster inputs"
    )
    prep_decay_2_pop.add_argument("in_df", type=file_exists, help="input data frame")
    prep_decay_2_pop.add_argument(
        "out_dir", type=parent_exists, help="path to save input npz"
    )

    decay = subparsers.add_parser("ld_decay", help="compute ld decay")
    decay.add_argument("stat", type=str, help="stat to compute")
    decay.add_argument("in_ts", type=file_exists, help="input tree sequence")
    decay.add_argument("in_arr", type=file_exists, help="input numpy array")
    decay.add_argument("out_arr", type=parent_exists, help="output numpy array")
    decay.add_argument(
        "--threads", type=int, default=1, help="number of threads to use"
    )

    decay_2_pop = subparsers.add_parser("ld_decay_2_pop", help="compute ld decay")
    decay_2_pop.add_argument("stat", type=str, help="stat to compute")
    # Hack for now.. path
    decay_2_pop.add_argument("in_ts", type=Path, help="input tree sequence")
    decay_2_pop.add_argument("in_arr", type=file_exists, help="input numpy array")
    decay_2_pop.add_argument("out_arr", type=parent_exists, help="output numpy array")
    decay_2_pop.add_argument(
        "--threads", type=int, default=1, help="number of threads to use"
    )
    return parser.parse_args()


args = parse_args()
log = logging.getLogger(__name__)
log.info("input parameters: %s", args)

match args.subcommand:
    case "compute_divergence":
        ts = load_ts(args.in_ts)
        df = pl.read_parquet(args.in_df)
        assert one(df["run_id"].unique()) in str(
            args.in_ts
        ), "df not associated with ts"
        log.info("computing divergence and geog distance")
        result = compute_divergence_and_geog_distance_for_sim(ts, df)
        log.info("writing %s", args.out_df)
        result.write_parquet(args.out_df)
        log.info("wrote %s", args.out_df)
    case "prepare_compute_divergence":
        _, df = read_parquet_file(args.in_df)
        assert isinstance(df, pl.LazyFrame)  # mypy
        partitions = (
            df.drop("sample_group", "ind", "x", "y", "age")
            .sort("run_id", "sampling_time", "s_ind")
            .collect()
            .partition_by(["run_id"], as_dict=True, maintain_order=True)
        )
        args.out_dir.mkdir()
        for (run_id,), d in partitions.items():
            d.write_parquet(args.out_dir / f"{run_id}.parquet")
    case "ld_decay":
        ts = load_ts(args.in_ts)
        samples = np.load(args.in_arr)["arr_0"]
        log.info("computing LD decay")
        decay = ld_decay(
            ts,
            max_dist=1_000_000,
            chunk_size=100,
            win_size=100,
            n_threads=args.threads,
            stat=args.stat,
            sample_sets=[samples],
        )
        np.savez(args.out_arr, decay)
        log.info("wrote %s", args.out_arr)
    case "ld_decay_2_pop":
        in_path = args.in_ts.with_name(
            # hack for now
            "-".join(args.in_ts.name.split("-")[0:3])
        ).with_suffix(".trees.tsz")
        ts = load_ts(args.in_ts)
        samples = np.load(args.in_arr)["arr_0"]
        assert samples.shape[0] == 2, "must be only 2 sample sets"
        log.info("computing LD decay (two pop)")
        bins, count, decay = ld_decay_two_way(
            ts,
            max_dist=1_000_000,
            chunk_size=100,
            win_size=100,
            n_threads=args.threads,
            stat=args.stat,
            sample_sets=samples,
        )
        np.savez(args.out_arr, decay=decay, bins=bins, count=count)
        log.info("wrote %s", args.out_arr)
    case "prepare_ld_decay":
        sampled = read_parquet_file(args.in_df, collect=True)
        assert isinstance(sampled, pl.DataFrame)  # mypy
        args.out_dir.mkdir()
        for run_id in sampled["run_id"].unique():
            sample_set = (
                sampled.filter(pl.col.run_id == run_id)["ind_nodes"]
                .to_numpy()
                .flatten()
            )
            np.savez(args.out_dir / run_id, sample_set)
    case "prepare_ld_decay_2_pop":
        meta, sampled = read_parquet_file(args.in_df, metadata=True, collect=True)
        assert isinstance(meta, pl.DataFrame)  # mypy
        assert isinstance(sampled, pl.DataFrame)  # mypy
        args.out_dir.mkdir()
        SD_selections = (
            list(range(5)) + list(range(6, 8, 2)) + list(range(9, 25, 3)) + [24]
        )
        SDs = meta["SD"].unique().sort()[SD_selections]
        sd_runs = meta.filter(pl.col.SD.is_in(SDs)).select("SD", "run_id")
        run_groups = (
            sampled.select("sampling_time", "run_id", "sample_group", "ind_nodes")
            .filter(pl.col.run_id.is_in(sd_runs["run_id"]))
            .join(sd_runs, on="run_id")
            .group_by(["run_id", "sample_group"])
            .agg(pl.col.ind_nodes)
            .partition_by(["run_id"], as_dict=True, include_key=False)
        )
        sample_sets = dict()
        for (run_id,), rg in run_groups.items():
            sample_groups = rg.partition_by(
                ["sample_group"], as_dict=True, include_key=False
            )
            left = sample_groups[(0,)]["ind_nodes"].explode().to_numpy().flatten()
            for (sample_group,), sg in sample_groups.items():
                sample_set = np.vstack(
                    [left, sg["ind_nodes"].explode().to_numpy().flatten()]
                )
                np.savez(args.out_dir / f"{run_id}-0-{sample_group}", sample_set)


log.info("complete")
